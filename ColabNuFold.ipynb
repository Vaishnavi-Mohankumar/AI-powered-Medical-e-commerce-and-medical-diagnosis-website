{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg3hmGrGv6Le"
      },
      "source": [
        "\n",
        "![nufold.png](https://github.com/kiharalab/nufold/blob/master/nufold/NuFoldLogo.png?raw=1)</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T0lLSCCRPTJ"
      },
      "source": [
        "NuFold is a state-of-the-art method designed for predicting 3D RNA structures, leveraging deep learning for high accuracy and reliability. This tool is particularly useful for biologists and bioinformatics researchers focusing on RNA function and structure.\n",
        "\n",
        "License: GPL v3 for academic use. (For commercial use, please contact us for different licensing) Contact: Daisuke Kihara (dkihara@purdue.edu)\n",
        "\n",
        "Cite: [Kagaya, Y., Zhang, Z., Ibtehaz, N. et al. NuFold: end-to-end approach for RNA tertiary structure prediction with flexible nucleobase center representation. Nat Commun 16, 881 (2025).](https://doi.org/10.1038/s41467-025-56261-7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koaiV0xzvMnr"
      },
      "source": [
        "# HOW TO USE NUFOLD\n",
        "- Below you will find a place to input your sequence and job name.\n",
        "- After giving sequence, go to the toolbar above, click `Runtime` and then `Run all`.\n",
        "- Change any settings you want to change before running.\n",
        "- **UPDATE:** We added AlphaFold3's MSA pipeline. You can also choose the databases to search!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKtL8aghuged",
        "outputId": "74dd642f-cbfa-4c1e-96ba-7e7caadd62d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNA sequence length: 89\n",
            "RNA sequence saved as jobs/4LVW_base/4LVW/4LVW.fasta of length: 89\n",
            "Selected MSA pipeline: AlphaFold3\n",
            "Detected 2 cores\n"
          ]
        }
      ],
      "source": [
        "# @title Input your RNA sequence and provide a job name (optional)\n",
        "# @markdown # Add your data here\n",
        "import random\n",
        "import string\n",
        "import os\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def random_string(length=10):\n",
        "    \"\"\"Generate a random string of fixed length.\"\"\"\n",
        "    letters = string.ascii_lowercase\n",
        "    return \"\".join(random.choice(letters) for i in range(length))\n",
        "\n",
        "\n",
        "rna_sequence = \"GGAGAGUAGAUGAUUCGCGUUAAGUGUGUGUGAAUGGGAUGUCGUCACACAACGAAGCGAGAGCGCGGUGAAUCAUUGCAUCCGCUCCA\"  # @param {type: \"string\"}\n",
        "length = len(rna_sequence)\n",
        "print(f\"RNA sequence length: {length}\")\n",
        "rna_sequence = rna_sequence.upper()\n",
        "job_name = \"4LVW\"  # @param {type: \"string\"}\n",
        "\n",
        "if not job_name:\n",
        "    job_name = random_string()\n",
        "\n",
        "# Prepare the FASTA content\n",
        "fasta_content = f\">{job_name}\\n{rna_sequence}\"\n",
        "jobs_dir = \"jobs\"\n",
        "if not os.path.exists(jobs_dir):\n",
        "    os.makedirs(jobs_dir)\n",
        "job_base_dir = os.path.join(jobs_dir, f\"{job_name}_base\")\n",
        "if not os.path.exists(job_base_dir):\n",
        "    os.makedirs(job_base_dir)\n",
        "actual_job_dir = os.path.join(job_base_dir, job_name)\n",
        "if not os.path.exists(actual_job_dir):\n",
        "    os.makedirs(actual_job_dir)\n",
        "fasta_filename = f\"{job_name}.fasta\"\n",
        "fasta_filepath = os.path.join(actual_job_dir, fasta_filename)\n",
        "with open(fasta_filepath, \"w\") as fasta_file:\n",
        "    fasta_file.write(fasta_content)\n",
        "\n",
        "print(f\"RNA sequence saved as {fasta_filepath} of length: {length}\")\n",
        "\n",
        "# @markdown If you want to download NuFold's output to your computer automatically, you can check this box here. Otherwise after the run is complete you can go to `/nufold_output` on the file explorer on the left side and download the folder with your job name.\n",
        "download = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ## MSA Pipeline Selection\n",
        "# @markdown You can choose which MSA pipeline to use, the options are:\n",
        "# @markdown - rMSA light (Without NT search)\n",
        "# @markdown - AlphaFold3 MSA Pipeline (nhmmer)\n",
        "\n",
        "# @markdown **NOTE: The free version of Google colab does not enough have enough space to install both rMSA and AlphaFold3 MSA pipelines, if you want to switch pipeline, change the pipeline and then do `Runtime` -> `Restart session and run all`**\n",
        "msa_pipeline = \"AlphaFold3\"  # @param [\"rMSA\", \"AlphaFold3\"] {type:\"string\"}\n",
        "print(f\"Selected MSA pipeline: {msa_pipeline}\")\n",
        "\n",
        "# @markdown ## Additional Settings:\n",
        "num_recycles = 3  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown ### rMSA Settings (if using rMSA pipeline):\n",
        "msa_speed = 1  # @param {type:\"slider\", min:0, max:2, step:1}\n",
        "# @markdown - 0 for slowest, 1 for normal, 2 for fast (Faster = Lower Quality)\n",
        "\n",
        "# @markdown ### AlphaFold3 MSA Settings (if using AlphaFold3 pipeline):\n",
        "# @markdown **NOTE: We let users choose what databases will be searched for the MSA, however with the free plan of Google colab, there is not enough disk space for the `ntrna` database. Please uncheck `use_ntrna_db` if you are using the free plan.**\n",
        "# @markdown - If you have Google colab pro you do not need to worry about this\n",
        "use_rfam_db = True  # @param {type:\"boolean\"}\n",
        "use_rnacentral_db = True  # @param {type:\"boolean\"}\n",
        "use_ntrna_db = False  # @param {type:\"boolean\"}\n",
        "max_sequences_per_db = 10000  # @param {type:\"number\"}\n",
        "e_value = 0.001  # @param {type:\"number\"}\n",
        "time_limit_minutes = 120  # @param {type:\"slider\", min:1, max:120, step:1}\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "print(f\"Detected {cores} cores\")\n",
        "# Create a dictionary of AF3 MSA database settings for later use\n",
        "af3_database_settings = {\n",
        "    \"Rfam\": use_rfam_db,\n",
        "    \"RNAcentral\": use_rnacentral_db,\n",
        "    \"NT_RNA\": use_ntrna_db,\n",
        "    \"time_limit_minutes\": time_limit_minutes,\n",
        "    \"max_sequences_per_db\": max_sequences_per_db,\n",
        "    \"e_value\": e_value,\n",
        "    \"n_cpu\": 2,  # Default to 2 cores for Colab\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e7SHqCosh31",
        "outputId": "9a69c3cb-c5b3-4449-a65e-c34babc23c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlphaFold3 MSA pipeline dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "# @title Installing Dependencies and setup (This can take up to 20 mins if using rMSA)\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "\n",
        "!rm -rf sample_data\n",
        "\n",
        "\n",
        "def run_command(command, log_file, message=None):\n",
        "    with open(log_file, \"a\") as file:\n",
        "        subprocess.run(command, shell=True, stdout=file, stderr=subprocess.STDOUT)\n",
        "    if message:\n",
        "        print(message)\n",
        "\n",
        "\n",
        "logs_file = \"logs.txt\"\n",
        "open(logs_file, \"w\").close()\n",
        "\n",
        "# Check and Install rMSA if not exists\n",
        "if msa_pipeline == \"rMSA\":\n",
        "    if not os.path.exists(\"rMSA\"):\n",
        "        run_command(\n",
        "            \"apt-get install aria2 && \"\n",
        "            \"git clone https://github.com/pylelab/rMSA && \"\n",
        "            \"cd rMSA/database/ && \"\n",
        "            \"aria2c -q -R -x 16 -j 20 -s 65536 -c --optimize-concurrent-downloads https://kiharalab.org/nufold/database.zip && \"\n",
        "            \"unzip database.zip && \"\n",
        "            \"rm database.zip && \"\n",
        "            \"cd ../../\",\n",
        "            logs_file,\n",
        "            \"rMSA installation complete.\",\n",
        "        )\n",
        "    else:\n",
        "        print(\"rMSA directory already exists. Skipping installation.\")\n",
        "\n",
        "# Install AlphaFold3 MSA pipeline dependencies\n",
        "if msa_pipeline == \"AlphaFold3\":\n",
        "    run_command(\n",
        "        \"apt-get update && \"\n",
        "        \"apt-get install -y hmmer zstd && \"\n",
        "        \"pip install biopython tqdm numpy absl-py\",\n",
        "        logs_file,\n",
        "        \"AlphaFold3 MSA pipeline dependencies installed.\",\n",
        "    )\n",
        "\n",
        "# Install libraries\n",
        "run_command(\n",
        "    \"pip install ml-collections dm-tree deepspeed protobuf scipy biopython py3dmol\",\n",
        "    logs_file,\n",
        "    \"Libraries installation complete.\",\n",
        ")\n",
        "\n",
        "# Check and Install ipknot if not exists\n",
        "if not os.path.exists(\"/content/ipknot\"):\n",
        "    run_command(\n",
        "        \"wget https://github.com/satoken/ipknot/releases/download/v1.1.0/ipknot-1.1.0-x86_64-linux.zip && \"\n",
        "        \"unzip ipknot-1.1.0-x86_64-linux.zip && \"\n",
        "        \"rm ipknot-1.1.0-x86_64-linux/README.md && \"\n",
        "        \"mv ipknot-1.1.0-x86_64-linux/ipknot /content && \"\n",
        "        \"rmdir ipknot-1.1.0-x86_64-linux && \"\n",
        "        \"rm ipknot-1.1.0-x86_64-linux.zip && \"\n",
        "        \"chmod +x /content/ipknot\",\n",
        "        logs_file,\n",
        "        \"ipknot installation complete.\",\n",
        "    )\n",
        "else:\n",
        "    print(\"ipknot already exists. Skipping installation.\")\n",
        "\n",
        "# Check and Setup NuFold if not exists\n",
        "# Make sure to add gitclone nufold here.\n",
        "if not os.path.exists(\"/content/run_nufold.py\"):\n",
        "    run_command(\n",
        "        \"git clone https://github.com/kiharalab/NuFold &&\"\n",
        "        \"mv /content/NuFold/nufold /content && \"\n",
        "        \"mv /content/NuFold/run_nufold.py /content && \"\n",
        "        \"rm -rf /content/NuFold\",\n",
        "        logs_file,\n",
        "        \"NuFold setup complete.\",\n",
        "    )\n",
        "else:\n",
        "    print(\"NuFold is already set up. Skipping installation.\")\n",
        "\n",
        "print(\"All installations and setups are complete. Check logs.txt for details.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "KDs4o5Bv35MI"
      },
      "outputs": [],
      "source": [
        "# @title Data Preprocess: Compute pseudoknots and perform MSA\n",
        "# @markdown This step is going to take a while, allow some time for this step (keep the page open)\n",
        "\n",
        "# Paths\n",
        "jobs_dir = \"jobs\"\n",
        "job_dir = os.path.join(jobs_dir, job_name)\n",
        "fasta_filepath = os.path.join(job_dir, fasta_filename)\n",
        "ss_output_path = os.path.join(job_dir, f\"{job_name}.ss\")\n",
        "\n",
        "# Change to the job directory and run ipknot\n",
        "%cd {actual_job_dir}\n",
        "# Run ipknot and capture output\n",
        "!../../../ipknot {fasta_filename} > {fasta_filename.replace('.fasta', '')}.ipknot.ss\n",
        "print(\"📋 iPknot secondary structure prediction saved\")\n",
        "\n",
        "%cd ../../..\n",
        "\n",
        "# Function definitions for AlphaFold3 MSA pipeline\n",
        "if msa_pipeline == \"AlphaFold3\":\n",
        "    # Define constants\n",
        "    RNA_CHAIN = \"polyribonucleotide\"\n",
        "    SHORT_SEQUENCE_CUTOFF = 50\n",
        "    DB_DIR = \"/tmp/rna_databases\"\n",
        "    os.makedirs(DB_DIR, exist_ok=True)\n",
        "\n",
        "    # Define database URLs\n",
        "    SOURCE = \"https://storage.googleapis.com/alphafold-databases/v3.0\"\n",
        "    RNA_DATABASE_INFO = {\n",
        "        \"rnacentral_active_seq_id_90_cov_80_linclust.fasta\": \"RNAcentral\",\n",
        "        \"nt_rna_2023_02_23_clust_seq_id_90_cov_80_rep_seq.fasta\": \"NT_RNA\",\n",
        "        \"rfam_14_9_clust_seq_id_90_cov_80_rep_seq.fasta\": \"Rfam\",\n",
        "    }\n",
        "\n",
        "    def download_with_progress(url, dest_path):\n",
        "        \"\"\"Download a file with a progress bar\"\"\"\n",
        "        try:\n",
        "            with urllib.request.urlopen(url) as response:\n",
        "                file_size = int(response.info().get(\"Content-Length\", 0))\n",
        "                desc = f\"Downloading {os.path.basename(dest_path)}\"\n",
        "                with tqdm(\n",
        "                    total=file_size, unit=\"B\", unit_scale=True, desc=desc\n",
        "                ) as pbar:\n",
        "                    with open(dest_path, \"wb\") as out_file:\n",
        "                        while True:\n",
        "                            buffer = response.read(8192)\n",
        "                            if not buffer:\n",
        "                                break\n",
        "                            out_file.write(buffer)\n",
        "                            pbar.update(len(buffer))\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error downloading {url}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def download_selected_databases(database_settings):\n",
        "        \"\"\"Download only the databases selected in the settings\"\"\"\n",
        "        selected_dbs = []\n",
        "        if database_settings.get(\"Rfam\", False):\n",
        "            selected_dbs.append(\"Rfam\")\n",
        "        if database_settings.get(\"RNAcentral\", False):\n",
        "            selected_dbs.append(\"RNAcentral\")\n",
        "        if database_settings.get(\"NT_RNA\", False):\n",
        "            selected_dbs.append(\"NT_RNA\")\n",
        "\n",
        "        if not selected_dbs:\n",
        "            print(\"⚠️ No databases selected for download!\")\n",
        "            return\n",
        "\n",
        "        # Convert selected_dbs to appropriate file names\n",
        "        selected_db_files = []\n",
        "        for db_file, db_key in RNA_DATABASE_INFO.items():\n",
        "            if db_key in selected_dbs:\n",
        "                selected_db_files.append(db_file)\n",
        "\n",
        "        if not selected_db_files:\n",
        "            print(\"⚠️ No databases selected for download!\")\n",
        "            return\n",
        "\n",
        "        print(\n",
        "            f\"🌐 Downloading {len(selected_db_files)} RNA databases: {', '.join([RNA_DATABASE_INFO[db] for db in selected_db_files])}\"\n",
        "        )\n",
        "\n",
        "        # Check if we already have the databases\n",
        "        missing_dbs = []\n",
        "        for db in selected_db_files:\n",
        "            db_path = os.path.join(DB_DIR, db)\n",
        "            if not os.path.exists(db_path) or os.path.getsize(db_path) == 0:\n",
        "                missing_dbs.append(db)\n",
        "\n",
        "        if not missing_dbs:\n",
        "            print(\"✅ All selected databases already downloaded.\")\n",
        "            return\n",
        "\n",
        "        # Create progress bar for overall process\n",
        "        with tqdm(\n",
        "            total=len(missing_dbs), desc=\"Overall progress\", unit=\"db\", position=0\n",
        "        ) as main_pbar:\n",
        "            # Download and decompress each database\n",
        "            for db in missing_dbs:\n",
        "                dest_path = os.path.join(DB_DIR, f\"{db}.zst\")\n",
        "                final_path = os.path.join(DB_DIR, db)\n",
        "\n",
        "                if os.path.exists(final_path) and os.path.getsize(final_path) > 0:\n",
        "                    print(f\"✅ {db} already exists, skipping.\")\n",
        "                    main_pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "                # Download the compressed file\n",
        "                print(f\"📥 Downloading {db} ({RNA_DATABASE_INFO[db]})...\")\n",
        "                url = f\"{SOURCE}/{db}.zst\"\n",
        "                if download_with_progress(url, dest_path):\n",
        "                    # Decompress with zstd\n",
        "                    print(f\"📦 Decompressing {db}...\")\n",
        "                    try:\n",
        "                        subprocess.run(\n",
        "                            [\"zstd\", \"--decompress\", \"-f\", dest_path, \"-o\", final_path],\n",
        "                            check=True,\n",
        "                        )\n",
        "                        print(f\"✅ Successfully processed {db}\")\n",
        "\n",
        "                        # Remove the compressed file\n",
        "                        os.remove(dest_path)\n",
        "                    except Exception as e:\n",
        "                        print(f\"❌ Error decompressing {db}: {e}\")\n",
        "\n",
        "                main_pbar.update(1)\n",
        "\n",
        "        # Final check to see what we have\n",
        "        existing_dbs = []\n",
        "        for db in selected_db_files:\n",
        "            db_path = os.path.join(DB_DIR, db)\n",
        "            if os.path.exists(db_path) and os.path.getsize(db_path) > 0:\n",
        "                existing_dbs.append(db)\n",
        "\n",
        "        if len(existing_dbs) == len(selected_db_files):\n",
        "            print(\"🎉 All selected databases downloaded and ready for use.\")\n",
        "        else:\n",
        "            print(\n",
        "                f\"⚠️ Downloaded {len(existing_dbs)}/{len(selected_db_files)} databases.\"\n",
        "            )\n",
        "            print(\n",
        "                f\"✅ Available: {', '.join([RNA_DATABASE_INFO[db] for db in existing_dbs])}\"\n",
        "            )\n",
        "            missing = [db for db in selected_db_files if db not in existing_dbs]\n",
        "            print(f\"❌ Missing: {', '.join([RNA_DATABASE_INFO[db] for db in missing])}\")\n",
        "\n",
        "    def create_query_fasta_file(sequence, path, linewidth=80):\n",
        "        \"\"\"Creates a fasta file with the sequence\"\"\"\n",
        "        with open(path, \"w\") as f:\n",
        "            f.write(\">query\\n\")\n",
        "            i = 0\n",
        "            while i < len(sequence):\n",
        "                f.write(f\"{sequence[i : (i + linewidth)]}\\n\")\n",
        "                i += linewidth\n",
        "\n",
        "    def run_command(cmd, cmd_name):\n",
        "        \"\"\"Run a command and handle errors\"\"\"\n",
        "        import logging\n",
        "\n",
        "        logging.info(f\"Running {cmd_name}: {cmd}\")\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            completed_process = subprocess.run(\n",
        "                cmd,\n",
        "                check=True,\n",
        "                stderr=subprocess.PIPE,\n",
        "                stdout=subprocess.PIPE,\n",
        "                text=True,\n",
        "            )\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            logging.error(f\"{cmd_name} failed.\\nstdout: {e.stdout}\\nstderr: {e.stderr}\")\n",
        "            raise RuntimeError(\n",
        "                f\"{cmd_name} failed\\nstdout: {e.stdout}\\nstderr: {e.stderr}\"\n",
        "            ) from e\n",
        "\n",
        "        end_time = time.time()\n",
        "        logging.info(f\"Finished {cmd_name} in {end_time - start_time:.3f} seconds\")\n",
        "        return completed_process\n",
        "\n",
        "    def parse_fasta(fasta_string):\n",
        "        \"\"\"Parse a FASTA string into sequences and descriptions\"\"\"\n",
        "        sequences = []\n",
        "        descriptions = []\n",
        "\n",
        "        lines = fasta_string.strip().split(\"\\n\")\n",
        "        current_seq = \"\"\n",
        "        current_desc = \"\"\n",
        "\n",
        "        for line in lines:\n",
        "            if line.startswith(\">\"):\n",
        "                if current_seq:  # Save the previous sequence\n",
        "                    sequences.append(current_seq)\n",
        "                    descriptions.append(current_desc)\n",
        "                current_desc = line[1:].strip()  # Remove the '>' character\n",
        "                current_seq = \"\"\n",
        "            else:\n",
        "                current_seq += line.strip()\n",
        "\n",
        "        # Add the last sequence\n",
        "        if current_seq:\n",
        "            sequences.append(current_seq)\n",
        "            descriptions.append(current_desc)\n",
        "\n",
        "        return sequences, descriptions\n",
        "\n",
        "    def convert_stockholm_to_a3m(stockholm_path, max_sequences=None):\n",
        "        \"\"\"Convert Stockholm format MSA to A3M format\"\"\"\n",
        "        with open(stockholm_path, \"r\") as stockholm_file:\n",
        "            descriptions = {}\n",
        "            sequences = {}\n",
        "            reached_max_sequences = False\n",
        "\n",
        "            # First pass: extract sequences\n",
        "            for line in stockholm_file:\n",
        "                reached_max_sequences = (\n",
        "                    max_sequences and len(sequences) >= max_sequences\n",
        "                )\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith((\"#\", \"//\")):\n",
        "                    continue\n",
        "                seqname, aligned_seq = line.split(maxsplit=1)\n",
        "                if seqname not in sequences:\n",
        "                    if reached_max_sequences:\n",
        "                        continue\n",
        "                    sequences[seqname] = \"\"\n",
        "                sequences[seqname] += aligned_seq\n",
        "\n",
        "            if not sequences:\n",
        "                return \"\"\n",
        "\n",
        "            # Second pass: extract descriptions\n",
        "            stockholm_file.seek(0)\n",
        "            for line in stockholm_file:\n",
        "                line = line.strip()\n",
        "                if line[:4] == \"#=GS\":\n",
        "                    columns = line.split(maxsplit=3)\n",
        "                    seqname, feature = columns[1:3]\n",
        "                    value = columns[3] if len(columns) == 4 else \"\"\n",
        "                    if feature != \"DE\":\n",
        "                        continue\n",
        "                    if reached_max_sequences and seqname not in sequences:\n",
        "                        continue\n",
        "                    descriptions[seqname] = value\n",
        "                    if len(descriptions) == len(sequences):\n",
        "                        break\n",
        "\n",
        "        # Convert Stockholm to A3M\n",
        "        a3m_sequences = {}\n",
        "        query_sequence = next(iter(sequences.values()))\n",
        "        for seqname, sto_sequence in sequences.items():\n",
        "            # Align sequence to gapless query (simplified version)\n",
        "            a3m_seq = \"\"\n",
        "            query_idx = 0\n",
        "            for i, char in enumerate(sto_sequence):\n",
        "                if query_sequence[i] == \"-\":\n",
        "                    if char != \"-\":\n",
        "                        a3m_seq += char.lower()  # Add as lowercase (insertion)\n",
        "                else:  # Query has a residue here\n",
        "                    a3m_seq += char  # Add as is (match/mismatch/deletion)\n",
        "                    query_idx += 1\n",
        "            a3m_sequences[seqname] = a3m_seq.replace(\".\", \"\")\n",
        "\n",
        "        # Convert to FASTA format\n",
        "        fasta_chunks = []\n",
        "        for seqname, seq in a3m_sequences.items():\n",
        "            fasta_chunks.append(f\">{seqname} {descriptions.get(seqname, '')}\")\n",
        "            fasta_chunks.append(seq)\n",
        "\n",
        "        return \"\\n\".join(fasta_chunks) + \"\\n\"\n",
        "\n",
        "    class Nhmmer:\n",
        "        \"\"\"Python wrapper of the Nhmmer binary\"\"\"\n",
        "\n",
        "        def __init__(\n",
        "            self,\n",
        "            binary_path,\n",
        "            hmmalign_binary_path,\n",
        "            hmmbuild_binary_path,\n",
        "            database_path,\n",
        "            n_cpu=8,\n",
        "            e_value=1e-3,\n",
        "            max_sequences=10000,\n",
        "            alphabet=\"rna\",\n",
        "            time_limit_minutes=None,\n",
        "        ):\n",
        "            \"\"\"Initialize Nhmmer wrapper\"\"\"\n",
        "            self.binary_path = binary_path\n",
        "            self.hmmalign_binary_path = hmmalign_binary_path\n",
        "            self.hmmbuild_binary_path = hmmbuild_binary_path\n",
        "            self.db_path = database_path\n",
        "            self.e_value = e_value\n",
        "            self.n_cpu = n_cpu\n",
        "            self.max_sequences = max_sequences\n",
        "            self.alphabet = alphabet\n",
        "            self.time_limit_seconds = (\n",
        "                time_limit_minutes * 60 if time_limit_minutes else None\n",
        "            )\n",
        "\n",
        "        def query(self, target_sequence):\n",
        "            \"\"\"Query the database using Nhmmer and return results in A3M format\"\"\"\n",
        "            import logging\n",
        "            import time\n",
        "            import os\n",
        "            import tempfile\n",
        "            import subprocess\n",
        "\n",
        "            logging.info(f\"Querying database with sequence: {target_sequence[:20]}...\")\n",
        "\n",
        "            with tempfile.TemporaryDirectory() as query_tmp_dir:\n",
        "                input_fasta_path = os.path.join(query_tmp_dir, \"query.fasta\")\n",
        "                output_sto_path = os.path.join(query_tmp_dir, \"output.sto\")\n",
        "\n",
        "                # Create query FASTA file\n",
        "                create_query_fasta_file(sequence=target_sequence, path=input_fasta_path)\n",
        "\n",
        "                # Prepare Nhmmer command\n",
        "                cmd_flags = [\n",
        "                    \"-o\",\n",
        "                    \"/dev/null\",  # Don't pollute stdout\n",
        "                    \"--noali\",  # Don't include the alignment in stdout\n",
        "                    \"--cpu\",\n",
        "                    str(self.n_cpu),\n",
        "                    \"-E\",\n",
        "                    str(self.e_value),\n",
        "                    \"-A\",\n",
        "                    output_sto_path,\n",
        "                ]\n",
        "\n",
        "                # Add alphabet flag\n",
        "                if self.alphabet:\n",
        "                    cmd_flags.extend([f\"--{self.alphabet}\"])\n",
        "\n",
        "                # Special handling for short RNA sequences\n",
        "                if (\n",
        "                    self.alphabet == \"rna\"\n",
        "                    and len(target_sequence) < SHORT_SEQUENCE_CUTOFF\n",
        "                ):\n",
        "                    cmd_flags.extend([\"--F3\", str(0.02)])\n",
        "                else:\n",
        "                    cmd_flags.extend([\"--F3\", str(1e-5)])\n",
        "\n",
        "                # Add input and database paths\n",
        "                cmd_flags.extend([input_fasta_path, self.db_path])\n",
        "\n",
        "                # Setup progress monitoring\n",
        "                if self.time_limit_seconds is None:\n",
        "                    print(\n",
        "                        f\"⏳ Running Nhmmer search against {os.path.basename(self.db_path)} (no time limit)\"\n",
        "                    )\n",
        "                else:\n",
        "                    print(\n",
        "                        f\"⏳ Running Nhmmer search against {os.path.basename(self.db_path)} (time limit: {self.time_limit_seconds // 60} min)\"\n",
        "                    )\n",
        "\n",
        "                # Create a process with timeout\n",
        "                cmd = [self.binary_path, *cmd_flags]\n",
        "                start_time = time.time()\n",
        "\n",
        "                try:\n",
        "                    # Use subprocess with timeout\n",
        "                    process = subprocess.Popen(\n",
        "                        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "                    )\n",
        "\n",
        "                    # Create progress bar for time monitoring\n",
        "                    if self.time_limit_seconds is None:\n",
        "                        # If no time limit, just wait for the process to finish\n",
        "                        while process.poll() is None:\n",
        "                            time.sleep(5)  # Check periodically\n",
        "                            elapsed = time.time() - start_time\n",
        "                            # Update progress every minute\n",
        "                            if int(elapsed) % 60 == 0:\n",
        "                                print(\n",
        "                                    f\"⏳ Search in progress... (elapsed time: {elapsed:.0f} seconds)\"\n",
        "                                )\n",
        "                    else:\n",
        "                        # If time limit is set, use a progress bar\n",
        "                        with tqdm(\n",
        "                            total=self.time_limit_seconds,\n",
        "                            desc=\"Search time\",\n",
        "                            unit=\"sec\",\n",
        "                        ) as pbar:\n",
        "                            elapsed = 0\n",
        "                            while (\n",
        "                                process.poll() is None\n",
        "                                and elapsed < self.time_limit_seconds\n",
        "                            ):\n",
        "                                time.sleep(1)\n",
        "                                elapsed = time.time() - start_time\n",
        "                                pbar.update(1)\n",
        "                                pbar.set_description(\n",
        "                                    f\"Search time ({elapsed:.0f}/{self.time_limit_seconds} sec)\"\n",
        "                                )\n",
        "\n",
        "                            # If we hit the time limit, terminate the process\n",
        "                            if process.poll() is None:\n",
        "                                print(\n",
        "                                    f\"⚠️ Time limit reached ({self.time_limit_seconds} seconds). Terminating search.\"\n",
        "                                )\n",
        "                                process.terminate()\n",
        "                                process.wait()\n",
        "\n",
        "                                # Even with timeout, check if we got partial results\n",
        "                                if (\n",
        "                                    os.path.exists(output_sto_path)\n",
        "                                    and os.path.getsize(output_sto_path) > 0\n",
        "                                ):\n",
        "                                    print(\n",
        "                                        \"✅ Found partial results within the time limit.\"\n",
        "                                    )\n",
        "                                else:\n",
        "                                    print(\"❌ No results found within the time limit.\")\n",
        "                                    return f\">query\\n{target_sequence}\"\n",
        "\n",
        "                    # Get process status\n",
        "                    if process.poll() is None:  # Process is still running\n",
        "                        process.terminate()\n",
        "                        process.wait()\n",
        "\n",
        "                    # Get stdout/stderr\n",
        "                    stdout, stderr = process.communicate()\n",
        "\n",
        "                    # Report completion\n",
        "                    elapsed = time.time() - start_time\n",
        "                    print(f\"✅ Search completed in {elapsed:.2f} seconds\")\n",
        "\n",
        "                    if process.returncode != 0 and not (\n",
        "                        os.path.exists(output_sto_path)\n",
        "                        and os.path.getsize(output_sto_path) > 0\n",
        "                    ):\n",
        "                        print(f\"❌ Nhmmer failed with error: {stderr}\")\n",
        "                        return f\">query\\n{target_sequence}\"\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error running Nhmmer: {e}\")\n",
        "                    return f\">query\\n{target_sequence}\"\n",
        "\n",
        "                # Check if we got any hits\n",
        "                if (\n",
        "                    os.path.exists(output_sto_path)\n",
        "                    and os.path.getsize(output_sto_path) > 0\n",
        "                ):\n",
        "                    with open(output_sto_path, \"r\") as f:\n",
        "                        line_count = sum(1 for _ in f)\n",
        "                    print(f\"✅ Found hits in Stockholm file ({line_count} lines)\")\n",
        "\n",
        "                    # Build profile from query sequence\n",
        "                    print(\"🧬 Building HMM profile from query sequence...\")\n",
        "                    hmmbuild = Hmmbuild(\n",
        "                        binary_path=self.hmmbuild_binary_path, alphabet=self.alphabet\n",
        "                    )\n",
        "                    target_sequence_fasta = f\">query\\n{target_sequence}\\n\"\n",
        "                    profile = hmmbuild.build_profile_from_fasta(target_sequence_fasta)\n",
        "\n",
        "                    # Convert Stockholm to A3M\n",
        "                    print(\"📝 Converting Stockholm to A3M format...\")\n",
        "                    a3m_out = convert_stockholm_to_a3m(\n",
        "                        output_sto_path, max_sequences=self.max_sequences - 1\n",
        "                    )\n",
        "\n",
        "                    # Align hits to the query profile\n",
        "                    print(\"📊 Aligning sequences to query profile...\")\n",
        "                    aligner = Hmmalign(binary_path=self.hmmalign_binary_path)\n",
        "                    aligned_a3m = aligner.align_sequences_to_profile(\n",
        "                        profile=profile, sequences_a3m=a3m_out\n",
        "                    )\n",
        "\n",
        "                    # Get sequence count\n",
        "                    seq_count = aligned_a3m.count(\">\")\n",
        "                    print(f\"🎯 Successfully aligned {seq_count} sequences\")\n",
        "\n",
        "                    # Return A3M with query sequence first\n",
        "                    return \"\".join([target_sequence_fasta, aligned_a3m])\n",
        "                else:\n",
        "                    print(f\"⚠️ No hits found\")\n",
        "                    # No hits - return only query sequence\n",
        "                    return f\">query\\n{target_sequence}\"\n",
        "\n",
        "    class Hmmbuild:\n",
        "        \"\"\"Python wrapper for hmmbuild - construct HMM profiles from MSA\"\"\"\n",
        "\n",
        "        def __init__(self, binary_path, alphabet=None):\n",
        "            \"\"\"Initialize Hmmbuild wrapper\"\"\"\n",
        "            self.binary_path = binary_path\n",
        "            self.alphabet = alphabet\n",
        "\n",
        "        def build_profile_from_fasta(self, fasta):\n",
        "            \"\"\"Build an HMM profile from a FASTA string\"\"\"\n",
        "            import re\n",
        "            import tempfile\n",
        "            import os\n",
        "\n",
        "            # Process FASTA to remove inserted residues (lowercase letters)\n",
        "            sequences, descriptions = parse_fasta(fasta)\n",
        "            lines = []\n",
        "            for seq, desc in zip(sequences, descriptions):\n",
        "                # Remove inserted residues (lowercase)\n",
        "                seq = re.sub(\"[a-z]+\", \"\", seq)\n",
        "                lines.append(f\">{desc}\\n{seq}\\n\")\n",
        "            msa = \"\".join(lines)\n",
        "\n",
        "            with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "                input_msa_path = os.path.join(tmp_dir, \"query.msa\")\n",
        "                output_hmm_path = os.path.join(tmp_dir, \"output.hmm\")\n",
        "\n",
        "                with open(input_msa_path, \"w\") as f:\n",
        "                    f.write(msa)\n",
        "\n",
        "                # Prepare command\n",
        "                cmd_flags = [\"--informat\", \"afa\"]\n",
        "                if self.alphabet:\n",
        "                    cmd_flags.append(f\"--{self.alphabet}\")\n",
        "\n",
        "                cmd_flags.extend([output_hmm_path, input_msa_path])\n",
        "                cmd = [self.binary_path, *cmd_flags]\n",
        "\n",
        "                # Run hmmbuild\n",
        "                run_command(cmd=cmd, cmd_name=\"Hmmbuild\")\n",
        "\n",
        "                # Read the output profile\n",
        "                with open(output_hmm_path) as f:\n",
        "                    hmm = f.read()\n",
        "\n",
        "                return hmm\n",
        "\n",
        "    class Hmmalign:\n",
        "        \"\"\"Python wrapper of the hmmalign binary\"\"\"\n",
        "\n",
        "        def __init__(self, binary_path):\n",
        "            \"\"\"Initialize Hmmalign wrapper\"\"\"\n",
        "            self.binary_path = binary_path\n",
        "\n",
        "        def align_sequences_to_profile(self, profile, sequences_a3m):\n",
        "            \"\"\"Align sequences to a profile and return in A3M format\"\"\"\n",
        "            import tempfile\n",
        "            import os\n",
        "\n",
        "            # Process A3M to remove gaps\n",
        "            sequences, descriptions = parse_fasta(sequences_a3m)\n",
        "            lines = []\n",
        "            for seq, desc in zip(sequences, descriptions):\n",
        "                # Remove gaps\n",
        "                seq = seq.replace(\"-\", \"\")\n",
        "                lines.append(f\">{desc}\\n{seq}\\n\")\n",
        "            sequences_no_gaps_a3m = \"\".join(lines)\n",
        "\n",
        "            with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "                input_profile = os.path.join(tmp_dir, \"profile.hmm\")\n",
        "                input_seqs = os.path.join(tmp_dir, \"sequences.a3m\")\n",
        "                output_a3m_path = os.path.join(tmp_dir, \"output.a3m\")\n",
        "\n",
        "                with open(input_profile, \"w\") as f:\n",
        "                    f.write(profile)\n",
        "\n",
        "                with open(input_seqs, \"w\") as f:\n",
        "                    f.write(sequences_no_gaps_a3m)\n",
        "\n",
        "                # Prepare command\n",
        "                cmd = [\n",
        "                    self.binary_path,\n",
        "                    \"-o\",\n",
        "                    output_a3m_path,\n",
        "                    \"--outformat\",\n",
        "                    \"A2M\",  # A2M is A3M in the HMMER suite\n",
        "                    input_profile,\n",
        "                    input_seqs,\n",
        "                ]\n",
        "\n",
        "                # Run hmmalign\n",
        "                run_command(cmd=cmd, cmd_name=\"Hmmalign\")\n",
        "\n",
        "                # Read the aligned output\n",
        "                with open(output_a3m_path, encoding=\"utf-8\") as f:\n",
        "                    a3m = f.read()\n",
        "\n",
        "                return a3m\n",
        "\n",
        "    class Msa:\n",
        "        \"\"\"Multiple Sequence Alignment container with methods for manipulating it\"\"\"\n",
        "\n",
        "        def __init__(\n",
        "            self,\n",
        "            query_sequence,\n",
        "            chain_poly_type,\n",
        "            sequences,\n",
        "            descriptions,\n",
        "            deduplicate=True,\n",
        "        ):\n",
        "            \"\"\"Initialize MSA container\"\"\"\n",
        "            import string\n",
        "            import re\n",
        "\n",
        "            if len(sequences) != len(descriptions):\n",
        "                raise ValueError(\"The number of sequences and descriptions must match.\")\n",
        "\n",
        "            self.query_sequence = query_sequence\n",
        "            self.chain_poly_type = chain_poly_type\n",
        "\n",
        "            if not deduplicate:\n",
        "                self.sequences = sequences\n",
        "                self.descriptions = descriptions\n",
        "            else:\n",
        "                self.sequences = []\n",
        "                self.descriptions = []\n",
        "                # A replacement table that removes all lowercase characters\n",
        "                deletion_table = str.maketrans(\"\", \"\", string.ascii_lowercase)\n",
        "                unique_sequences = set()\n",
        "                for seq, desc in zip(sequences, descriptions):\n",
        "                    # Using string.translate is faster than re.sub('[a-z]+', '')\n",
        "                    sequence_no_deletions = seq.translate(deletion_table)\n",
        "                    if sequence_no_deletions not in unique_sequences:\n",
        "                        unique_sequences.add(sequence_no_deletions)\n",
        "                        self.sequences.append(seq)\n",
        "                        self.descriptions.append(desc)\n",
        "\n",
        "            # Make sure the MSA always has at least the query\n",
        "            self.sequences = self.sequences or [query_sequence]\n",
        "            self.descriptions = self.descriptions or [\"Original query\"]\n",
        "\n",
        "            # Check if the 1st MSA sequence matches the query sequence\n",
        "            if not self._sequences_are_feature_equivalent(\n",
        "                self.sequences[0], query_sequence\n",
        "            ):\n",
        "                raise ValueError(\n",
        "                    f\"First MSA sequence {self.sequences[0]} is not the query sequence {query_sequence}\"\n",
        "                )\n",
        "\n",
        "        def _sequences_are_feature_equivalent(self, sequence1, sequence2):\n",
        "            \"\"\"Check if two sequences are equivalent (ignoring insertions)\"\"\"\n",
        "            import re\n",
        "\n",
        "            # For RNA, we can simply compare the uppercase versions\n",
        "            if self.chain_poly_type == RNA_CHAIN:\n",
        "                seq1_upper = re.sub(\"[a-z]+\", \"\", sequence1)\n",
        "                seq2_upper = re.sub(\"[a-z]+\", \"\", sequence2)\n",
        "                return seq1_upper == seq2_upper\n",
        "            return sequence1 == sequence2  # Fallback for other types\n",
        "\n",
        "        @classmethod\n",
        "        def from_multiple_msas(cls, msas, deduplicate=True):\n",
        "            \"\"\"Initialize MSA from multiple MSAs\"\"\"\n",
        "            if not msas:\n",
        "                raise ValueError(\"At least one MSA must be provided.\")\n",
        "\n",
        "            query_sequence = msas[0].query_sequence\n",
        "            chain_poly_type = msas[0].chain_poly_type\n",
        "            sequences = []\n",
        "            descriptions = []\n",
        "\n",
        "            for msa in msas:\n",
        "                if msa.query_sequence != query_sequence:\n",
        "                    raise ValueError(\n",
        "                        f\"Query sequences must match: {[m.query_sequence for m in msas]}\"\n",
        "                    )\n",
        "                if msa.chain_poly_type != chain_poly_type:\n",
        "                    raise ValueError(\n",
        "                        f\"Chain poly types must match: {[m.chain_poly_type for m in msas]}\"\n",
        "                    )\n",
        "                sequences.extend(msa.sequences)\n",
        "                descriptions.extend(msa.descriptions)\n",
        "\n",
        "            return cls(\n",
        "                query_sequence=query_sequence,\n",
        "                chain_poly_type=chain_poly_type,\n",
        "                sequences=sequences,\n",
        "                descriptions=descriptions,\n",
        "                deduplicate=deduplicate,\n",
        "            )\n",
        "\n",
        "        @classmethod\n",
        "        def from_a3m(\n",
        "            cls, query_sequence, chain_poly_type, a3m, max_depth=None, deduplicate=True\n",
        "        ):\n",
        "            \"\"\"Parse a single A3M and build the Msa object\"\"\"\n",
        "            sequences, descriptions = parse_fasta(a3m)\n",
        "\n",
        "            if max_depth is not None and 0 < max_depth < len(sequences):\n",
        "                print(\n",
        "                    f\"MSA cropped from depth of {len(sequences)} to {max_depth} for {query_sequence}\"\n",
        "                )\n",
        "                sequences = sequences[:max_depth]\n",
        "                descriptions = descriptions[:max_depth]\n",
        "\n",
        "            return cls(\n",
        "                query_sequence=query_sequence,\n",
        "                chain_poly_type=chain_poly_type,\n",
        "                sequences=sequences,\n",
        "                descriptions=descriptions,\n",
        "                deduplicate=deduplicate,\n",
        "            )\n",
        "\n",
        "        @property\n",
        "        def depth(self):\n",
        "            \"\"\"Return the number of sequences in the MSA\"\"\"\n",
        "            return len(self.sequences)\n",
        "\n",
        "        def to_a3m(self):\n",
        "            \"\"\"Return the MSA in A3M format\"\"\"\n",
        "            a3m_lines = []\n",
        "            for desc, seq in zip(self.descriptions, self.sequences):\n",
        "                a3m_lines.append(f\">{desc}\")\n",
        "                a3m_lines.append(seq)\n",
        "            return \"\\n\".join(a3m_lines) + \"\\n\"\n",
        "\n",
        "    def af3_generate_rna_msa(rna_sequence, database_settings):\n",
        "        \"\"\"Generate MSA for an RNA sequence using AlphaFold3's pipeline\"\"\"\n",
        "        import time\n",
        "        import urllib.request\n",
        "        import os\n",
        "\n",
        "        # Validate RNA sequence\n",
        "        rna_sequence = rna_sequence.upper().strip()\n",
        "        valid_bases = set(\"ACGU\")\n",
        "        if not all(base in valid_bases for base in rna_sequence):\n",
        "            raise ValueError(\n",
        "                f\"Invalid RNA sequence. Must contain only A, C, G, U: {rna_sequence}\"\n",
        "            )\n",
        "\n",
        "        print(\n",
        "            f\"🧪 Validating RNA sequence: {rna_sequence[:20]}...\"\n",
        "            + (\n",
        "                \"\"\n",
        "                if len(rna_sequence) <= 20\n",
        "                else f\"... ({len(rna_sequence)} nucleotides)\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Setup paths to binaries and databases\n",
        "        nhmmer_binary = \"/usr/bin/nhmmer\"\n",
        "        hmmalign_binary = \"/usr/bin/hmmalign\"\n",
        "        hmmbuild_binary = \"/usr/bin/hmmbuild\"\n",
        "\n",
        "        database_paths = {\n",
        "            \"Rfam\": os.path.join(\n",
        "                DB_DIR, \"rfam_14_9_clust_seq_id_90_cov_80_rep_seq.fasta\"\n",
        "            ),\n",
        "            \"RNAcentral\": os.path.join(\n",
        "                DB_DIR, \"rnacentral_active_seq_id_90_cov_80_linclust.fasta\"\n",
        "            ),\n",
        "            \"NT_RNA\": os.path.join(\n",
        "                DB_DIR, \"nt_rna_2023_02_23_clust_seq_id_90_cov_80_rep_seq.fasta\"\n",
        "            ),\n",
        "        }\n",
        "\n",
        "        # Download selected databases if they don't exist\n",
        "        selected_dbs = []\n",
        "        if database_settings.get(\"Rfam\", False):\n",
        "            selected_dbs.append(\"Rfam\")\n",
        "        if database_settings.get(\"RNAcentral\", False):\n",
        "            selected_dbs.append(\"RNAcentral\")\n",
        "        if database_settings.get(\"NT_RNA\", False):\n",
        "            selected_dbs.append(\"NT_RNA\")\n",
        "\n",
        "        # Download any missing databases that are selected\n",
        "        missing_dbs = []\n",
        "        for db_key in selected_dbs:\n",
        "            db_filename = None\n",
        "            for filename, key in RNA_DATABASE_INFO.items():\n",
        "                if key == db_key:\n",
        "                    db_filename = filename\n",
        "                    break\n",
        "\n",
        "            if db_filename:\n",
        "                db_path = os.path.join(DB_DIR, db_filename)\n",
        "                if not os.path.exists(db_path) or os.path.getsize(db_path) == 0:\n",
        "                    missing_dbs.append(db_key)\n",
        "\n",
        "        if missing_dbs:\n",
        "            print(f\"⚠️ Some selected databases are missing: {', '.join(missing_dbs)}\")\n",
        "            print(\"📥 Downloading missing databases...\")\n",
        "            download_selected_databases(database_settings)\n",
        "            print(\"Continuing with pipeline execution...\")\n",
        "\n",
        "        # Check which databases actually exist now\n",
        "        existing_dbs = []\n",
        "        with tqdm(\n",
        "            total=len(database_paths), desc=\"Checking databases\", unit=\"db\"\n",
        "        ) as pbar:\n",
        "            for db_name, db_path in database_paths.items():\n",
        "                if os.path.exists(db_path) and os.path.getsize(db_path) > 0:\n",
        "                    db_key = RNA_DATABASE_INFO.get(os.path.basename(db_path))\n",
        "                    if db_key:\n",
        "                        existing_dbs.append(db_key)\n",
        "                pbar.update(1)\n",
        "\n",
        "        # Check if any selected database exists\n",
        "        selected_existing_dbs = [\n",
        "            db for db in existing_dbs if database_settings.get(db, False)\n",
        "        ]\n",
        "\n",
        "        if not selected_existing_dbs:\n",
        "            print(\n",
        "                \"⚠️ No selected databases could be downloaded or found. Continuing with query sequence only.\"\n",
        "            )\n",
        "            return f\">query\\n{rna_sequence}\\n\"\n",
        "        else:\n",
        "            print(\n",
        "                f\"📊 Found {len(selected_existing_dbs)} selected databases: {', '.join(selected_existing_dbs)}\"\n",
        "            )\n",
        "\n",
        "        # Generate MSA\n",
        "        print(\"🚀 Starting MSA generation...\")\n",
        "\n",
        "        print(f\"Getting RNA MSAs for sequence: {rna_sequence[:20]}...\")\n",
        "        print(\n",
        "            f\"🧬 Starting MSA search for RNA sequence of length {len(rna_sequence)}...\"\n",
        "        )\n",
        "        rna_msa_start_time = time.time()\n",
        "\n",
        "        # Extract settings\n",
        "        time_limit_minutes = database_settings.get(\"time_limit_minutes\")\n",
        "        max_sequences_per_db = database_settings.get(\"max_sequences_per_db\", 10000)\n",
        "        n_cpu = database_settings.get(\"n_cpu\", 2)  # Default to 2 for Colab\n",
        "        e_value = database_settings.get(\"e_value\", 0.001)\n",
        "\n",
        "        # Filter database paths based on settings and check if files exist\n",
        "        filtered_db_paths = {}\n",
        "        for db_name, db_path in database_paths.items():\n",
        "            db_key = None\n",
        "            for file_name, key in RNA_DATABASE_INFO.items():\n",
        "                if file_name in db_path:\n",
        "                    db_key = key\n",
        "                    break\n",
        "\n",
        "            # Check if database is selected in settings AND file exists with content\n",
        "            if (\n",
        "                db_key\n",
        "                and database_settings.get(db_key, False)\n",
        "                and os.path.exists(db_path)\n",
        "                and os.path.getsize(db_path) > 0\n",
        "            ):\n",
        "                filtered_db_paths[db_name] = db_path\n",
        "\n",
        "        # Setup progress tracking\n",
        "        total_dbs = len(filtered_db_paths)\n",
        "        if total_dbs == 0:\n",
        "            print(\"❌ No selected databases found or none selected in settings.\")\n",
        "            return f\">query\\n{rna_sequence}\\n\"\n",
        "\n",
        "        time_limit_str = (\n",
        "            \"no time limit\"\n",
        "            if time_limit_minutes is None\n",
        "            else f\"{time_limit_minutes} minutes per database\"\n",
        "        )\n",
        "        print(f\"🔍 Will search {total_dbs} databases with {time_limit_str}\")\n",
        "        progress_bar = tqdm(total=total_dbs, desc=\"Database searches\", unit=\"db\")\n",
        "\n",
        "        # Run Nhmmer on each database\n",
        "        msas = []\n",
        "        for db_name, db_path in filtered_db_paths.items():\n",
        "            print(f\"🔍 Searching database: {os.path.basename(db_path)}...\")\n",
        "            nhmmer_runner = Nhmmer(\n",
        "                binary_path=nhmmer_binary,\n",
        "                hmmalign_binary_path=hmmalign_binary,\n",
        "                hmmbuild_binary_path=hmmbuild_binary,\n",
        "                database_path=db_path,\n",
        "                n_cpu=n_cpu,\n",
        "                e_value=e_value,\n",
        "                max_sequences=max_sequences_per_db,\n",
        "                alphabet=\"rna\",\n",
        "                time_limit_minutes=time_limit_minutes,\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                a3m_result = nhmmer_runner.query(rna_sequence)\n",
        "                msa = Msa.from_a3m(\n",
        "                    query_sequence=rna_sequence,\n",
        "                    chain_poly_type=RNA_CHAIN,\n",
        "                    a3m=a3m_result,\n",
        "                    deduplicate=False,\n",
        "                )\n",
        "                msas.append(msa)\n",
        "                print(f\"✅ Found {msa.depth} sequences in {db_name}\")\n",
        "                progress_bar.update(1)\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing {db_name}: {e}\")\n",
        "                progress_bar.update(1)\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "        # Merge and deduplicate MSAs\n",
        "        print(\"🔄 Merging and deduplicating sequences from all databases...\")\n",
        "        if not msas:\n",
        "            # If all searches failed, create an empty MSA with just the query\n",
        "            rna_msa = Msa(\n",
        "                query_sequence=rna_sequence,\n",
        "                chain_poly_type=RNA_CHAIN,\n",
        "                sequences=[rna_sequence],\n",
        "                descriptions=[\"Original query\"],\n",
        "                deduplicate=False,\n",
        "            )\n",
        "            print(\n",
        "                \"⚠️ No homologous sequences found. MSA contains only the query sequence.\"\n",
        "            )\n",
        "            a3m = f\">query\\n{rna_sequence}\\n\"\n",
        "        else:\n",
        "            rna_msa = Msa.from_multiple_msas(msas=msas, deduplicate=True)\n",
        "            print(\n",
        "                f\"🎉 MSA construction complete! Found {rna_msa.depth} unique sequences.\"\n",
        "            )\n",
        "            a3m = rna_msa.to_a3m()\n",
        "\n",
        "        elapsed_time = time.time() - rna_msa_start_time\n",
        "        print(f\"⏱️ Total MSA generation time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        return a3m\n",
        "\n",
        "\n",
        "# Run the chosen MSA pipeline based on user selection\n",
        "if msa_pipeline == \"rMSA\":\n",
        "    print(\"🧬 Running rMSA pipeline...\")\n",
        "    # Run rMSA.pl\n",
        "    seqpath = os.path.join(actual_job_dir, fasta_filename)\n",
        "    !sed -i 's@$dbdir/nt@@g' rMSA/rMSA.pl\n",
        "    !rMSA/rMSA.pl {seqpath} -cpu=2 -fast={msa_speed}\n",
        "    print(\"✅ rMSA.pl run completed.\")\n",
        "else:  # AlphaFold3 pipeline\n",
        "    print(\"🧬 Running AlphaFold3 MSA pipeline...\")\n",
        "    import urllib.request\n",
        "    import time\n",
        "\n",
        "    # Run the MSA pipeline\n",
        "    af3_msa = af3_generate_rna_msa(rna_sequence, af3_database_settings)\n",
        "\n",
        "    # Save the MSA to the same location that rMSA would save to\n",
        "    msa_output_path = os.path.join(actual_job_dir, f\"{job_name}.afa\")\n",
        "    with open(msa_output_path, \"w\") as f:\n",
        "        f.write(af3_msa)\n",
        "    # Gzip the MSA file to match rMSA output format\n",
        "    msa_gzip_path = os.path.join(actual_job_dir, f\"{job_name}.b.afa.gz\")\n",
        "    import gzip\n",
        "\n",
        "    with open(msa_output_path, \"rb\") as f_in:\n",
        "        with gzip.open(msa_gzip_path, \"wb\") as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "\n",
        "    # Display sequence count\n",
        "    seq_count = af3_msa.count(\">\")\n",
        "    print(f\"✅ AlphaFold3 MSA generation complete! Found {seq_count} sequences.\")\n",
        "    print(f\"✅ MSA saved to {msa_gzip_path}\")\n",
        "\n",
        "    # Create a symlink to make the MSA file available where NuFold expects it\n",
        "    if not os.path.exists(f\"{actual_job_dir}/{job_name}.a.afa.gz\"):\n",
        "        !cp {msa_gzip_path} {actual_job_dir}/{job_name}.a.afa.gz\n",
        "\n",
        "    print(\"✅ AlphaFold3 MSA pipeline completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "woN1sckydWRP"
      },
      "outputs": [],
      "source": [
        "# @title Predict with NuFold\n",
        "\n",
        "if not os.path.exists(\"checkpoints\"):\n",
        "    os.makedirs(\"checkpoints\")\n",
        "    !wget -O checkpoints/global_step145245.pt http://kiharalab.org/nufold/global_step145245.pt\n",
        "else:\n",
        "    print('\"checkpoints\" already fetched')\n",
        "\n",
        "seq = os.path.join(actual_job_dir, fasta_filename)\n",
        "\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import time\n",
        "import threading\n",
        "import sys\n",
        "\n",
        "\n",
        "def spinner_animation():\n",
        "    spinner = [\"⠋\", \"⠙\", \"⠹\", \"⠸\", \"⠼\", \"⠴\", \"⠦\", \"⠧\", \"⠇\", \"⠏\"]\n",
        "    i = 0\n",
        "    while not prediction_done:\n",
        "        sys.stdout.write(\"\\r\" + f\"Running NuFold prediction... {spinner[i]} \")\n",
        "        sys.stdout.flush()\n",
        "        i = (i + 1) % len(spinner)\n",
        "        time.sleep(0.1)\n",
        "    sys.stdout.write(\"\\r\" + \"NuFold prediction done!                 \\n\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "# Create output directory silently\n",
        "!mkdir -p nufold_output > /dev/null 2>&1\n",
        "\n",
        "# Start spinner in a separate thread\n",
        "prediction_done = False\n",
        "spinner_thread = threading.Thread(target=spinner_animation)\n",
        "spinner_thread.start()\n",
        "\n",
        "# Run the prediction\n",
        "!python /content/run_nufold.py \\\n",
        "  --ckpt_path /content/checkpoints/global_step145245.pt \\\n",
        "  --input_fasta {seq} \\\n",
        "  --input_dir {job_base_dir}\\\n",
        "  --output_dir /content/nufold_output \\\n",
        "  --config_preset initial_training \\\n",
        "  --recycle {num_recycles} > /dev/null 2>&1\n",
        "\n",
        "# Signal that prediction is done\n",
        "prediction_done = True\n",
        "spinner_thread.join()\n",
        "\n",
        "# Display success message with some styling\n",
        "display(\n",
        "    HTML(\n",
        "        '<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border-radius: 5px; font-weight: bold; text-align: center;\">✅ NuFold prediction completed successfully!</div>'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "orMwqpBrbf38"
      },
      "outputs": [],
      "source": [
        "# @title Display Result\n",
        "import os\n",
        "import py3Dmol\n",
        "from IPython.display import display, Markdown, Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming output_path and fasta_filename are defined as shown previously\n",
        "output_path = os.path.join(\n",
        "    \"/content/nufold_output\", fasta_filename.replace(\".fasta\", \"\")\n",
        ")\n",
        "files = os.listdir(output_path)\n",
        "\n",
        "# Filter for the PDB files with the specific prefix\n",
        "pdb_files = [x for x in files if x.startswith(job_name) and x.endswith(\".pdb\")]\n",
        "pdb_files.sort()  # Sort PDB files by name\n",
        "\n",
        "# We'll only create one plot\n",
        "plddt_plot_filename = f\"{job_name}_plddt_plot.png\"\n",
        "\n",
        "\n",
        "# Function to get pLDDT scores from PDB file\n",
        "def get_plddt_scores(pdb_file_path):\n",
        "    plddt_scores = []\n",
        "    with open(pdb_file_path) as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"ATOM\"):\n",
        "                try:\n",
        "                    # pLDDT scores are stored in the B-factor column (positions 61-66)\n",
        "                    plddt = float(line[60:66].strip())\n",
        "                    plddt_scores.append(plddt)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    return plddt_scores\n",
        "\n",
        "\n",
        "# Function to visualize PDB file with py3Dmol with improved styling\n",
        "def visualize_pdb_file(pdb_file_path, width=800, height=500):\n",
        "    with open(pdb_file_path) as ifile:\n",
        "        system = \"\".join([x for x in ifile])\n",
        "    view = py3Dmol.view(width=width, height=height)\n",
        "    view.addModelsAsFrames(system)\n",
        "\n",
        "    # Apply improved styling with confidence coloring\n",
        "    view.setStyle(\n",
        "        {\n",
        "            \"cartoon\": {\n",
        "                \"colorscheme\": {\"prop\": \"b\", \"gradient\": \"roygb\", \"min\": 50, \"max\": 90},\n",
        "                \"thickness\": 1.0,\n",
        "                \"opacity\": 0.9,\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Set high-quality rendering\n",
        "    view.setViewStyle({\"style\": \"outline\"})\n",
        "    view.zoomTo()\n",
        "\n",
        "    return view\n",
        "\n",
        "\n",
        "# Function to create the pLDDT plot\n",
        "def create_plddt_plot(pdb_files, output_dir, width=12, height=8, dpi=120):\n",
        "    plt.figure(figsize=(width, height), dpi=dpi)\n",
        "\n",
        "    # Define a colormap for the plot lines\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, min(10, len(pdb_files))))\n",
        "\n",
        "    for i, pdb_file in enumerate(pdb_files):\n",
        "        pdb_path = os.path.join(output_dir, pdb_file)\n",
        "        plddt_scores = get_plddt_scores(pdb_path)\n",
        "\n",
        "        if plddt_scores:\n",
        "            residue_indices = range(1, len(plddt_scores) + 1)\n",
        "            plt.plot(\n",
        "                residue_indices,\n",
        "                plddt_scores,\n",
        "                label=f\"Model {i + 1}\",\n",
        "                color=colors[i],\n",
        "                linewidth=2.5,\n",
        "                alpha=0.8,\n",
        "            )\n",
        "\n",
        "    # Add confidence level regions with improved transparency\n",
        "    plt.axhspan(90, 100, alpha=0.2, color=\"blue\", label=\"Very high confidence\")\n",
        "    plt.axhspan(70, 90, alpha=0.2, color=\"green\", label=\"High confidence\")\n",
        "    plt.axhspan(50, 70, alpha=0.2, color=\"yellow\", label=\"Medium confidence\")\n",
        "    plt.axhspan(0, 50, alpha=0.2, color=\"red\", label=\"Low confidence\")\n",
        "\n",
        "    plt.title(\n",
        "        f\"Confidence Scores by Nucleotide Position for {job_name}\", fontsize=18, pad=20\n",
        "    )\n",
        "    plt.xlabel(\"Nucleotide Position\", fontsize=16)\n",
        "    plt.ylabel(\"pLDDT Score\", fontsize=16)\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
        "    plt.legend(loc=\"upper right\", fontsize=14, framealpha=0.9)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Add tick marks and ensure y-axis goes from 0 to 100\n",
        "    plt.xlim(1, max(len(get_plddt_scores(os.path.join(output_dir, pdb_files[0]))), 1))\n",
        "    plt.ylim(0, 100)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    # Save the plot with the standard filename\n",
        "    plot_path = os.path.join(output_dir, plddt_plot_filename)\n",
        "    plt.savefig(plot_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    return plot_path\n",
        "\n",
        "\n",
        "# If there are PDB files, analyze and visualize them\n",
        "if pdb_files:\n",
        "    # Create the pLDDT plot only once\n",
        "    plot_path = create_plddt_plot(pdb_files, output_path)\n",
        "\n",
        "    display(Markdown(f\"# 🧬 NuFold Structure Predictions for {job_name}\"))\n",
        "\n",
        "    # Display 3D models\n",
        "\n",
        "    # @markdown **Color scheme represents confidence (pLDDT):**\n",
        "    # @markdown - **Blue**: Very high confidence (90-100)\n",
        "    # @markdown - **Green**: High confidence (70-90)\n",
        "    # @markdown - **Yellow**: Medium confidence (50-70)\n",
        "    # @markdown - **Orange/Red**: Low confidence (0-50)\n",
        "\n",
        "    # Set the number of models to display (can be made adjustable)\n",
        "    # @markdown Number of structures to display:\n",
        "    num_structures = 1  # @param {\"type\":\"raw\",\"placeholder\":\"1\"}\n",
        "    num_structures = min(num_structures, len(pdb_files))\n",
        "\n",
        "    for i, pdb_file in enumerate(pdb_files[:num_structures]):\n",
        "        display(Markdown(f\"### Model {i + 1}\"))\n",
        "        view = visualize_pdb_file(os.path.join(output_path, pdb_file))\n",
        "        view.show()\n",
        "\n",
        "    # Display pLDDT plot at the end\n",
        "    display(Markdown(\"## 📈 Per-Nucleotide Confidence Plot\"))\n",
        "    # display(Image(plot_path))\n",
        "\n",
        "else:\n",
        "    display(Markdown(\"# ⚠️ No Results Found\"))\n",
        "    display(\n",
        "        Markdown(\n",
        "            \"No PDB files were generated for your job. There might have been an error during processing.\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Code for downloading the ZIP file if needed\n",
        "zipname = fasta_filename.replace(\".fasta\", \"\")\n",
        "if download:\n",
        "    import shutil\n",
        "    from google.colab import files\n",
        "\n",
        "    zip_filename = f\"{zipname}.zip\"\n",
        "    shutil.make_archive(zip_filename.replace(\".zip\", \"\"), \"zip\", output_path)\n",
        "    display(Markdown(\"## The download is probably going to take a while.\"))\n",
        "    files.download(zip_filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}